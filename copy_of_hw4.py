# -*- coding: utf-8 -*-
"""Copy of hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MY9TMTCyt8hj1PhR-1GTCHFfPzFSi6OB
"""

import os
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
root_path = os.path.join(os.getcwd(), "drive", "My Drive/mnist") # replace based on your Google drive organization
dataset_path = os.path.join(root_path, "data") # same here

!pip install python-mnist
from mnist.loader import MNIST

import os
import numpy
from numpy import random
import scipy
import matplotlib
import mnist
import pickle
import time
from matplotlib import pyplot
import keras
import tensorflow as tf
mnist = tf.keras.datasets.mnist
from keras import models
from keras import layers
import numpy as np
from keras.layers.normalization import BatchNormalization
from keras.layers import Activation

### hyperparameter settings and other constants
batch_size = 128
num_classes = 10
epochs = 10
mnist_input_shape = (28, 28, 1)
d1 = 1024
d2 = 256
alpha = 0.1
beta = 0.9
alpha_adam = 0.001
rho1 = 0.99
rho2 = 0.999
### end hyperparameter settings


# load the MNIST dataset using TensorFlow/Keras
def load_MNIST_dataset():
    mnist = tf.keras.datasets.mnist
    (Xs_tr, Ys_tr), (Xs_te, Ys_te) = mnist.load_data()
    Xs_tr = Xs_tr / 255.0
    Xs_te = Xs_te / 255.0
    Xs_tr = Xs_tr.reshape(Xs_tr.shape[0], 28, 28, 1) # 28 rows, 28 columns, 1 channel
    Xs_te = Xs_te.reshape(Xs_te.shape[0], 28, 28, 1)
    return (Xs_tr, Ys_tr, Xs_te, Ys_te)


# #convert y lable to vector form
# def categorical(Ys):
#     cat_Ys=np.zeros(shape=[len(Ys),10])
#     for i in range(len(Ys)):
#         index=Ys[i]
#         cat_Ys[i][index]=1
#     return cat_Ys


# evaluate a trained model on MNIST data, and print the usual output from TF
#
# Xs        examples to evaluate on
# Ys        labels to evaluate on
# model     trained model
#
# returns   tuple of (loss, accuracy)
def evaluate_model(Xs, Ys, model):
    (loss, accuracy) = model.evaluate(Xs, Ys)
    return (loss, accuracy)


# train a fully connected two-hidden-layer neural network on MNIST data using SGD, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# d1        the size of the first layer
# d2        the size of the second layer
# alpha     step size parameter
# beta      momentum parameter (0.0 if no momentum)
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_fully_connected_sgd(Xs, Ys, d1, d2, alpha, beta, B, epochs):
    model = keras.Sequential()
    Xs = tf.squeeze(Xs)
    model.add(keras.layers.Flatten(input_shape=(28,28)))           
    model.add(keras.layers.Dense(units=d1, activation=tf.nn.relu)) 
    model.add(keras.layers.Dense(units=d2, activation=tf.nn.relu)) 
    model.add(keras.layers.Dense(units=10, activation=tf.nn.softmax))
    opt = tf.keras.optimizers.SGD(learning_rate=alpha)
    model.compile(opt,loss="sparse_categorical_crossentropy",metrics=["sparse_categorical_accuracy"])
    history = model.fit(x=Xs,y=Ys,epochs=epochs,batch_size=B,validation_split = 0.1)
    
    return (model, history)
   
    # TODO students should implement this
    

# train a fully connected two-hidden-layer neural network on MNIST data using Adam, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# d1        the size of the first layer
# d2        the size of the second layer
# alpha     step size parameter
# rho1      first moment decay parameter
# rho2      second moment decay parameter
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_fully_connected_adam(Xs, Ys, d1, d2, alpha, rho1, rho2, B, epochs):
    # TODO students should implement this
    model = keras.Sequential()
    Xs = tf.squeeze(Xs)
    model.add(keras.layers.Flatten(input_shape=(28,28)))           
    model.add(keras.layers.Dense(units=d1, activation=tf.nn.relu)) 
    model.add(keras.layers.Dense(units=d2, activation=tf.nn.relu)) 
    model.add(keras.layers.Dense(units=10, activation=tf.nn.softmax))
    opt = tf.keras.optimizers.Adam(learning_rate=alpha, beta_1=rho1, beta_2=rho2, epsilon=1e-07, amsgrad=False,name='Adam')
    model.compile(opt,loss="sparse_categorical_crossentropy",metrics=["sparse_categorical_accuracy"])
    history = model.fit(x=Xs,y=Ys,epochs=epochs,batch_size=B,validation_split = 0.1)
    return (model, history)

# train a fully connected two-hidden-layer neural network with Batch Normalization on MNIST data using SGD, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# d1        the size of the first layer
# d2        the size of the second layer
# alpha     step size parameter
# beta      momentum parameter (0.0 if no momentum)
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_fully_connected_bn_sgd(Xs, Ys, d1, d2, alpha, beta, B, epochs):
    # TODO students should implement this
    model = keras.Sequential()
    Xs = tf.squeeze(Xs)
    model.add(keras.layers.Flatten(input_shape=(28,28)))           
    model.add(keras.layers.Dense(units=d1)) 
    model.add(BatchNormalization(trainable=True))
    model.add(Activation('relu'))
    model.add(keras.layers.Dense(units=d2)) 
    model.add(BatchNormalization(trainable=True))
    model.add(Activation('relu'))
    model.add(keras.layers.Dense(units=10, activation=tf.nn.softmax))
    
    opt = tf.keras.optimizers.SGD(learning_rate=alpha, momentum=beta)
    model.compile(opt,loss="sparse_categorical_crossentropy",metrics=["sparse_categorical_accuracy"])
    history = model.fit(x=Xs,y=Ys,epochs=epochs,batch_size=B,validation_split = 0.1)
    return (model, history)


# train a convolutional neural network on MNIST data using SGD, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# alpha     step size parameter
# rho1      first moment decay parameter
# rho2      second moment decay parameter
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_CNN_sgd(Xs, Ys, alpha, rho1, rho2, B, epochs):
    # TODO students should implement this
    model=keras.Sequential()
    # model.add(keras.layers.Flatten(input_shape=(28,28,1)))  
    model.add(keras.layers.Conv2D(filters=32,kernel_size = 5, padding = 'same',activation = tf.nn.relu,input_shape = (28,28,1)))
    model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding = 'valid'))
    model.add(keras.layers.Conv2D(filters=64,kernel_size = 5, padding = 'same',activation = tf.nn.relu))
    model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding = 'valid'))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(units=512,activation = tf.nn.relu))
    model.add(keras.layers.Dense(units=10,activation = tf.nn.softmax))
    opt = tf.keras.optimizers.Adam(learning_rate=alpha, beta_1=rho1, beta_2=rho2, epsilon=1e-07, amsgrad=False,name='Adam')
    model.compile(opt,loss="sparse_categorical_crossentropy",metrics=["sparse_categorical_accuracy"])
    history = model.fit(x=Xs,y=Ys,epochs=epochs,batch_size=B,validation_split = 0.1)
    return (model, history)





if __name__ == "__main__":
    (Xs_tr, Ys_tr, Xs_te, Ys_te) = load_MNIST_dataset()
    # TODO students should add code to generate plots here

model_sgd, history_sgd = train_fully_connected_sgd(Xs_tr, Ys_tr, d1, d2, alpha, beta, 128, epochs)

test_loss_sgd,test_acc_sgd = evaluate_model(Xs_te, Ys_te, model_sgd)
print("Test Accuracy %.2f"%test_acc_sgd)

model_adam, history_adam = train_fully_connected_adam(Xs_tr, Ys_tr, d1, d2, 0.001, rho1, rho2, 128, epochs)

test_loss_adam,test_acc_adam = evaluate_model(Xs_te, Ys_te, model_adam)
print("Test Accuracy %.2f"%test_acc_adam)

model_bn, history_bn = train_fully_connected_bn_sgd(Xs_tr, Ys_tr, d1, d2, 0.001, beta, 128, epochs)

model_bn.trainable = False
test_loss_bn,test_acc_bn = evaluate_model(Xs_te, Ys_te, model_bn)
print("Test Accuracy %.2f"%test_acc_bn)

for i in range(10):
    power=np.random.randint(5,9,1)
    batch_rand=2**power
    epoch_rand=np.random.randint(10,31,1)
    beta_rand=np.random.uniform(0.5,1,1)
    
    print(batch_rand,epoch_rand,beta_rand)
    model_sgd_mom, history_sgd_mom = train_fully_connected_sgd(Xs_tr, Ys_tr, d1, d2, 0.1, beta_rand[0], batch_rand[0], epoch_rand[0])
    val_acc =  history_sgd_mom.history['val_sparse_categorical_accuracy']
    print("accuracy=",val_acc[-1])

power=np.random.randint(5,9,1)
power

"""### PART 3"""

start_time = time.time()
model_cnn, history_cnn = train_CNN_sgd(Xs_tr, Ys_tr, 0.001, rho1, rho2, 128, epochs)
wall_clock_time = time.time() - start_time
wall_times.append(("CNN train time" , wall_clock_time))

wall_times = []
wall_times.append(("CNN train time" , wall_clock_time))

wall_times

test_loss_cnn,test_acc_cnn = evaluate_model(Xs_te, Ys_te, model_cnn)
print("Test Accuracy for CNN %.2f"%test_acc_cnn)

import pandas as pd
def gather_stat(model, history):

    df = pd.DataFrame(history.history)
    df.columns = ['training loss', 'training acc', 'validation loss', 'validation acc']
    df.index.names = ['Epoch']
    
    test_loss,test_acc = evaluate_model(Xs_te, Ys_te, model)

#     test_result = [("Test loss",test_loss), ("Test acc",test_acc)]
    
    print("\n")
    print("##"*50)
    print("EPOCH DETAILS:")
    
    print(df)
    print("##"*50)
    print("TEST RESULT:")
    
    print("Test loss",test_loss)
    print("Test acc",test_acc)

    print("##"*50)
    print("WALL CLOCK TIME:")
    
    print(wall_clock_time)

gather_stat(model_cnn, history_cnn)

"""# 2 Analyze

## Fully_connected_sgd
"""

# Commented out IPython magic to ensure Python compatibility.
import keras
import matplotlib
# %matplotlib inline
from matplotlib import pyplot as plt
loss_sgd = history_sgd.history['loss']
val_loss_sgd =  history_sgd.history['val_loss']
epochs = range(1, len(loss_sgd)+1)
# plt.figure(figsize=(15,15))
plt.plot(epochs, loss_sgd, label='Training Loss')
plt.plot(epochs, val_loss_sgd, label='Val Loss')
plt.plot(epochs, [test_loss_sgd]*10, label ='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title("Loss for Train & Val & Test (SGD)")
plt.legend()

plt.show()
plt.close('all')

train_acc_sgd = history_sgd.history['sparse_categorical_accuracy']
val_acc_sgd =  history_sgd.history['val_sparse_categorical_accuracy']
epochs = range(1, len(loss_sgd)+1)

plt.plot(epochs, train_acc_sgd, label='Training Acc')
plt.plot(epochs, val_acc_sgd, label='Val Acc')
plt.plot(epochs, [test_acc_sgd]*10, label ='Test Acc')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title("Acc Train & Val & Test (SGD)")
plt.legend()

plt.show()
plt.close('all')

"""## Fully_connected_adam"""

loss_adam = history_adam.history['loss']
val_loss_adam =  history_adam.history['val_loss']
epochs = range(1, len(loss_adam)+1)
# plt.figure(figsize=(15,15))
plt.plot(epochs, loss_adam, label='Training Loss')
plt.plot(epochs, val_loss_adam, label='Val Loss')
plt.plot(epochs, [test_loss_adam]*10, label ='Test Acc')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title("Loss for Train & Val (Adam)")
plt.legend()

plt.show()
plt.close('all')

train_acc_adam = history_adam.history['sparse_categorical_accuracy']
val_acc_adam =  history_adam.history['val_sparse_categorical_accuracy']
epochs = range(1, len(loss_adam)+1)

plt.plot(epochs, train_acc_adam, label='Training Acc')
plt.plot(epochs, val_acc_adam, label='Val Acc')
plt.plot(epochs, [test_acc_adam]*10, label ='Test Acc')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title("Acc Train & Val (Adam)")
plt.legend()

plt.show()
plt.close('all')

"""## Fully_connected_BN"""

loss_bn = history_bn.history['loss']
val_loss_bn =  history_bn.history['val_loss']
epochs = range(1, len(loss_bn)+1)
# plt.figure(figsize=(15,15))
plt.plot(epochs, loss_bn, label='Training Loss')
plt.plot(epochs, val_loss_bn, label='Val Loss')
plt.plot(epochs, [test_loss_bn]*10, label ='Test Acc')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title("Loss for Train & Val & Test (BN)")
plt.legend()

plt.show()
plt.close('all')

train_acc_bn = history_bn.history['sparse_categorical_accuracy']
val_acc_bn =  history_bn.history['val_sparse_categorical_accuracy']
epochs = range(1, len(loss_bn)+1)

plt.plot(epochs, train_acc_bn, label='Training Acc')
plt.plot(epochs, val_acc_bn, label='Val Acc')
plt.plot(epochs, [test_acc_bn]*10, label ='Test Acc')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title("Acc Train & Val & Test (BN)")
plt.legend()

plt.show()
plt.close('all')

"""## CNN"""

# Commented out IPython magic to ensure Python compatibility.
import keras
import matplotlib
# %matplotlib inline
from matplotlib import pyplot as plt
loss_cnn = history_cnn.history['loss']
val_loss_cnn =  history_cnn.history['val_loss']
epochs = range(1, len(loss_cnn)+1)
# plt.figure(figsize=(15,15))
 
plt.plot(epochs, loss_cnn, label='Training Loss')
plt.plot(epochs, val_loss_cnn, label='Val Loss')
plt.plot(epochs, [test_loss_cnn]*10, label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title("Loss for Train & Val & Test (CNN)")
plt.legend()

plt.show()
plt.close('all')

train_acc_cnn = history_cnn.history['sparse_categorical_accuracy']
val_acc_cnn =  history_cnn.history['val_sparse_categorical_accuracy']
epochs = range(1, len(loss_cnn)+1)

plt.plot(epochs, train_acc_cnn, label='Training Acc')
plt.plot(epochs, val_acc_cnn, label='Val Acc')
plt.plot(epochs, [test_acc_cnn]*10, label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.title("Acc Train & Val & Test (CNN)")
plt.legend()

plt.show()
plt.close('all')

"""## Search

"""

def model_builder(hp):
  model = keras.Sequential()
  model.add(keras.layers.Flatten(input_shape=(28, 28)))

  # Tune the number of units in the first Dense layer
  # Choose an optimal value between 32-512
  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
  model.add(keras.layers.Dense(units=hp_units, activation='relu'))
  model.add(keras.layers.Dense(10))

  # Tune the learning rate for the optimizer
  # Choose an optimal value from 0.01, 0.001, or 0.0001
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

  return model

def build_model1(hp):
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28, 28)))
    #Xs = tf.squeeze(Xs)
    #model.add(keras.layers.Flatten(input_shape=(28,28)))           
    model.add(keras.layers.Dense(units=1024, activation=tf.nn.relu)) 
    model.add(keras.layers.Dense(units=256, activation=tf.nn.relu))
    model.add(keras.layers.Dense(units=10, activation=tf.nn.softmax))
    opt = tf.keras.optimizers.SGD( hp.Choice('learning_rate', values = [1.0,0.3,0.1,0.03,0.01,0.003,0.001], default = 0.1), momentum=beta)
    model.compile(opt,loss="sparse_categorical_crossentropy",metrics=["sparse_categorical_accuracy"])
    #history = model.fit(x=Xs,y=Ys,epochs=epochs,batch_size=128,validation_split = 0.1)
    
    return model

!pip install -q -U keras-tuner

import kerastuner as kt

tuner1 = kt.Hyperband(
    build_model1,
    objective='val_sparse_categorical_accuracy',
    max_epochs=10,
    hyperband_iterations=2)

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

tuner1.search(Xs_tr, Ys_tr, epochs=10, validation_split=0.1, callbacks=[stop_early])

# Get the optimal hyperparameters
best_hps=tuner1.get_best_hyperparameters(num_trials=1)[0]